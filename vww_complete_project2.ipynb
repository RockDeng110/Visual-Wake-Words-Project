{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedfd198",
   "metadata": {},
   "source": [
    "# End-to-End Visual Wake Words (VWW) Project for Edge AI\n",
    "\n",
    "## 1. Project Overview\n",
    "**Visual Wake Words (VWW)** represents a class of tiny, low-power computer vision models designed to detect the presence of a specific object (usually a person) to \"wake up\" a larger system. Think of it as the visual equivalent of \"Hey Siri\".\n",
    "\n",
    "**Real-World Use Cases include:**\n",
    "* **Smart Doorbells:** Only start recording/streaming when a human is at the door.\n",
    "* **Smart Office:** Turn on lights/HVAC only when a room is occupied.\n",
    "* **Safety Monitors:** Detect if a person enters a hazardous zone.\n",
    "\n",
    "### 1.1 Why Edge AI? (Advantages over Cloud)\n",
    "Traditionally, deep learning runs on powerful cloud servers. However, for \"always-on\" applications, moving inference to the **Edge (On-Device)** offers critical advantages:\n",
    "\n",
    "1.  **Privacy:** Images are processed locally and immediately discarded. No personal video feeds are streamed to the cloud, mitigating privacy concerns.\n",
    "2.  **Latency:** Inference happens in milliseconds without network round-trip delays. This is crucial for real-time safety triggers.\n",
    "3.  **Bandwidth & Power:** Streaming 24/7 video drains batteries and consumes massive data. A VWW model consumes milliwatts and sends only a simple \"wake up\" signal.\n",
    "4.  **Reliability:** The system functions correctly even without an internet connection.\n",
    "\n",
    "### 1.2 Key Challenges in Tiny Deep Learning\n",
    "Deploying Deep Learning models on Microcontrollers (MCUs) is significantly harder than on GPUs due to the **\"Iron Triangle\" of constraints**:\n",
    "\n",
    "*   **Extreme Memory Scarcity:** \n",
    "    *   Cloud GPUs have 24GB+ VRAM. \n",
    "    *   MCUs (e.g., ESP32, Arduino Nano) often have **<512KB SRAM** and **<2MB Flash**. \n",
    "    *   *Challenge:* The model must be tiny enough to fit in Flash and its runtime variables (activations) must fit in SRAM.\n",
    "*   **Limited Compute Power:** \n",
    "    *   MCUs run at low clock speeds (e.g., 80-240 MHz) with no hardware acceleration.\n",
    "    *   *Challenge:* We must minimize **MACs (Multiply-Accumulate Operations)** to ensure the inference finishes quickly (low latency).\n",
    "*   **Quantization Sensitivity:**\n",
    "    *   To save space and speed up math, we convert 32-bit floating-point numbers to **8-bit integers (Int8)**.\n",
    "    *   *Challenge:* This loss of precision can severely degrade accuracy if the model architecture is not robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a0de9",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 **Project Goals & Phased Approach**\n",
    "This project aims to not just train a single model, but to systematically evaluate and select the optimal architecture for a specific hardware constraint. We adopt a two-phase approach:\n",
    "\n",
    "**Phase 1: Model Design & Proxy Evaluation (Current Scope)**\n",
    "*   **Environment:** PC / Cloud (Google Colab / Local Jupyter).\n",
    "*   **Objective:** Train candidate models, perform Int8 quantization, and evaluate using \"Proxy Metrics\".\n",
    "*   **Key Metrics:** \n",
    "    *   **Accuracy / F1-Score / AUC:** To ensure detection reliability.\n",
    "    *   **Model Size (Flash Usage):** To ensure it fits on the chip.\n",
    "    *   **MACs (Multiply-Accumulate Operations):** A hardware-agnostic proxy for Latency and Power consumption.\n",
    "    *   **Peak RAM Estimation:** To ensure it fits in SRAM.\n",
    "\n",
    "**Phase 2: Hardware-in-the-Loop Deployment and evaluation(Future Work)**\n",
    "*   **Environment:** Physical MCU (e.g., ESP32-S3, Arduino Nano 33 BLE).\n",
    "*   **Objective:** Deploy the best candidates from Phase 1 and measure real-world performance.\n",
    "*   **Key Metrics:** \n",
    "    *   **Real Latency (ms):** Measured on-device.\n",
    "    *   **Real Power Consumption (mW):** Measured via power profiler.\n",
    "    *   **Real MACs**\n",
    "    *   **Real Peak RAM consumption**\n",
    "\n",
    "\n",
    "**Workflow in phase 1:**\n",
    "1.  **Data Engineering:** Acquire INRIA Person Dataset and perform EDA.\n",
    "2.  **Modeling:** Train MobileNetV2, MobileNetV3, and SimpleCNN.\n",
    "3.  **Quantization:** Convert to TFLite Int8.\n",
    "4.  **Evaluation:** Compare models using Accuracy, F1, AUC, and MACs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ce5d9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup\n",
    "We use TensorFlow 2.x. Either CPU, TPU or GPU is suitable for this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7bb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Check Environment\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Define Global Parameters\n",
    "IMG_SIZE = 96\n",
    "BATCH_SIZE = 32\n",
    "dataset_dir = pathlib.Path(\"vww_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815ace9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3. Data Engineering\n",
    "### Dataset Selection: INRIA Person Dataset\n",
    "For this project, we selected the **INRIA Person Dataset**, a benchmark dataset originally designed for pedestrian detection.\n",
    "\n",
    "#### **Why this dataset?**\n",
    "1.  **Real-World Complexity:** Unlike simple datasets (like MNIST or Fashion-MNIST), INRIA features people in diverse environments (cities, mountains, beaches) with varying lighting, poses, and occlusions. This forces the model to learn robust features.\n",
    "2.  **Relative Balanced Classes:** It provides a clear separation between \"Positive\" (images containing upright people) and \"Negative\" (images of scenery, buildings, cars, etc. with **no** people).\n",
    "3.  **Manageable Size:** At ~970MB, it is large enough to train a generalized model but small enough to download and process quickly in a Colab environment, unlike the 20GB+ COCO dataset often used for VWW.\n",
    "\n",
    "#### **Dataset Specs**\n",
    "*   **Source:** Dalal, N., & Triggs, B. (2005). *Histograms of oriented gradients for human detection*. CVPR.\n",
    "*   **Content:**\n",
    "    *   **Positive:** Images of standing people (pedestrians).\n",
    "    *   **Negative:** Diverse background images (urban, nature, indoor) without people.\n",
    "*   **Original Format:** Variable resolution images. We will resize them to 96x96.\n",
    "\n",
    "### 3.1 Download & Extraction\n",
    "We download the dataset from a stable archive mirror."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3d0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_url = \"https://web.archive.org/web/20190301110434/ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar\"\n",
    "archive_path = \"INRIAPerson.tar\"\n",
    "extract_path = \"INRIAPerson_Raw\"\n",
    "\n",
    "# 1. Download\n",
    "if not os.path.exists(archive_path) and not os.path.exists(extract_path):\n",
    "    print(\"Downloading INRIA Person Dataset (this may take a few minutes)...\")\n",
    "    # Using wget with no-check-certificate for stability\n",
    "    if os.system(f\"wget --no-check-certificate {dataset_url} -O {archive_path}\") != 0:\n",
    "        print(\"wget failed, trying urllib...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(dataset_url, archive_path)\n",
    "            print(\"Download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "else:\n",
    "    print(\"Archive already exists or extracted.\")\n",
    "\n",
    "# 2. Extract\n",
    "if not os.path.exists(extract_path):\n",
    "    if os.path.exists(archive_path):\n",
    "        print(\"Extracting...\")\n",
    "        try:\n",
    "            with tarfile.open(archive_path) as tar:\n",
    "                tar.extractall(extract_path)\n",
    "            print(\"Extraction complete.\")\n",
    "        except tarfile.ReadError:\n",
    "            print(\"Error: Corrupted archive.\")\n",
    "    else:\n",
    "        print(\"Archive not found.\")\n",
    "else:\n",
    "    print(\"Data already extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2847541",
   "metadata": {},
   "source": [
    "### 3.2 Data Restructuring\n",
    "We reorganize the raw data into the standard Keras directory format:\n",
    "*   `train/person` & `train/not_person`\n",
    "*   `validation/person` & `validation/not_person`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b1d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_dataset(source_root, dest_root):\n",
    "    if dest_root.exists():\n",
    "        print(f\"Destination {dest_root} already exists. Skipping organization.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Organizing data from {source_root} to {dest_root}...\")\n",
    "    inria_root = pathlib.Path(source_root) / \"INRIAPerson\"\n",
    "    \n",
    "    splits = {'train': 'Train', 'validation': 'Test'}\n",
    "    labels = {'person': 'pos', 'not_person': 'neg'}\n",
    "\n",
    "    for split_name, inria_split in splits.items():\n",
    "        for label_name, inria_label in labels.items():\n",
    "            src_dir = inria_root / inria_split / inria_label\n",
    "            dest_dir = dest_root / split_name / label_name\n",
    "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            files = list(src_dir.glob('*.png')) + list(src_dir.glob('*.jpg'))\n",
    "            for f in files:\n",
    "                shutil.copy2(f, dest_dir / f.name)\n",
    "                \n",
    "    print(\"Data organization complete.\")\n",
    "\n",
    "organize_dataset(extract_path, dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6aecc",
   "metadata": {},
   "source": [
    "### 3.3 Exploratory Data Analysis (EDA)\n",
    "Before preprocessing, it is crucial to understand our data distribution. We will check for class imbalance and visualize random samples to understand the difficulty of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452a8f8",
   "metadata": {},
   "source": [
    "#### 3.3.1 Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfe735bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Class Distribution Analysis\n",
    "def plot_class_distribution(data_dir):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    splits = ['train', 'validation']\n",
    "    labels = ['person', 'not_person']\n",
    "    \n",
    "    stats = []\n",
    "    for split in splits:\n",
    "        for label in labels:\n",
    "            count = len(list((data_dir / split / label).glob('*')))\n",
    "            stats.append({'Split': split, 'Label': label, 'Count': count})\n",
    "            \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(len(stats))\n",
    "    counts = [s['Count'] for s in stats]\n",
    "    names = [f\"{s['Split']}/{s['Label']}\" for s in stats]\n",
    "    colors = ['#4CAF50' if 'person' in n and 'not_person' not in n else '#F44336' for n in names]\n",
    "    \n",
    "    plt.bar(x, counts, color=colors)\n",
    "    plt.xticks(x, names)\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.ylabel('Number of Images')\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + 50, str(v), ha='center')\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac24887",
   "metadata": {},
   "source": [
    "#### 3.3.2 Image Quality & Dimensions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c65c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Image Quality & Dimensions Analysis\n",
    "def analyze_image_dimensions(data_dir):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    # Sample 500 images to speed up analysis\n",
    "    all_files = list(data_dir.glob('*/*/*'))\n",
    "    if len(all_files) > 500:\n",
    "        sampled_files = np.random.choice(all_files, 500, replace=False)\n",
    "    else:\n",
    "        sampled_files = all_files\n",
    "        \n",
    "    widths = []\n",
    "    heights = []\n",
    "    aspect_ratios = []\n",
    "    \n",
    "    print(\"Analyzing image dimensions (Sample: 500)...\")\n",
    "    for f in sampled_files:\n",
    "        try:\n",
    "            img = tf.io.decode_image(tf.io.read_file(str(f)))\n",
    "            h, w, _ = img.shape\n",
    "            widths.append(w)\n",
    "            heights.append(h)\n",
    "            aspect_ratios.append(w / h)\n",
    "        except:\n",
    "            pass # Skip corrupted\n",
    "            \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(widths, heights, alpha=0.5)\n",
    "    plt.title('Image Dimensions (Width vs Height)')\n",
    "    plt.xlabel('Width')\n",
    "    plt.ylabel('Height')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(aspect_ratios, bins=30, color='purple', alpha=0.7)\n",
    "    plt.title('Aspect Ratio Distribution')\n",
    "    plt.xlabel('Aspect Ratio (W/H)')\n",
    "    plt.axvline(1.0, color='k', linestyle='--', label='Square (1:1)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Check for extremely small images that might lose info when resized\n",
    "    small_imgs = sum(1 for w, h in zip(widths, heights) if w < 96 or h < 96)\n",
    "    plt.bar(['> 96x96', '< 96x96'], [len(widths)-small_imgs, small_imgs], color=['green', 'orange'])\n",
    "    plt.title('Resolution Check (Target: 96x96)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Analysis Summary:\")\n",
    "    print(f\"  - Mean Resolution: {np.mean(widths):.0f}x{np.mean(heights):.0f}\")\n",
    "    print(f\"  - Images smaller than target (96x96): {small_imgs} ({(small_imgs/len(widths))*100:.1f}%)\")\n",
    "    print(f\"  - Aspect Ratio skew: Most images are {'Portrait' if np.mean(aspect_ratios) < 1 else 'Landscape'}.\")\n",
    "    print(f\"  - Recommendation: {'Resize with padding' if np.std(aspect_ratios) > 0.5 else 'Direct resize'} is acceptable.\")\n",
    "\n",
    "analyze_image_dimensions(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dda065",
   "metadata": {},
   "source": [
    "#### 3.3.3 Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32c96025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 3. Sample Visualization\n",
    "def visualize_samples(data_dir, num_samples=5):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Positive Samples\n",
    "    pos_files = list((data_dir / 'train' / 'person').glob('*'))\n",
    "    neg_files = list((data_dir / 'train' / 'not_person').glob('*'))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Pos\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        if pos_files:\n",
    "            img = plt.imread(str(np.random.choice(pos_files)))\n",
    "            plt.imshow(img)\n",
    "        plt.title(\"Person (Pos)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Neg\n",
    "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "        if neg_files:\n",
    "            img = plt.imread(str(np.random.choice(neg_files)))\n",
    "            plt.imshow(img)\n",
    "        plt.title(\"Background (Neg)\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Random Samples from Training Set\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ca94d",
   "metadata": {},
   "source": [
    "### 3.4 Preprocessing Pipeline\n",
    "*   **Resize:** 96x96 (TinyML Standard Resolution).\n",
    "*   **Augmentation:** Random flips and brightness adjustments to improve robustness against real-world variations.\n",
    "*   **Normalization Strategy:** \n",
    "    *   We do **NOT** normalize in the pipeline. The pipeline outputs images in `[0, 255]`.\n",
    "    *   Normalization is handled **inside the model** via a `Rescaling` layer. This allows us to swap models easily (e.g., MobileNetV2 needs `[-1, 1]`, V3 handles it internally) without changing the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a84df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir / 'train',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir / 'validation',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Preprocessing Functions\n",
    "# NOTE: We removed normalization here to handle it inside the models.\n",
    "# This allows us to use models that expect different ranges (e.g., V3 expects [0,255], V2 expects [-1,1]).\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    return image, label\n",
    "\n",
    "# Build Pipelines\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.cache().shuffle(1000).map(augment, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"✅ Data Pipeline Ready (Output range: [0, 255]).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489c54c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Development\n",
    "\n",
    "Choosing the right architecture for Edge AI is a critical trade-off between **Accuracy**, **Latency**, **Model Size** and **Power consumption**. We evaluated three primary candidates and considered several others.\n",
    "\n",
    "**1. MobileNetV2**\n",
    "*   **Why we chose it:** MobileNetV2 is the \"Goldilocks\" of TinyML. It introduces *Inverted Residuals* and *Linear Bottlenecks*, which are highly efficient.\n",
    "*   **Configuration:** `alpha=0.35`. This width multiplier reduces the number of filters by ~65%, drastically shrinking the model size to fit within our <1MB constraint while maintaining ~94% accuracy.\n",
    "*   **Pros:** Mature support in TFLite Micro, excellent accuracy-to-size ratio, easy to fine-tune.\n",
    "\n",
    "**2. MobileNetV3-Small**\n",
    "*   **Overview:** The successor to V2, designed specifically for mobile phones using Neural Architecture Search (NAS).\n",
    "*   **Configuration:** We use `minimalistic=True`. This replaces advanced activation functions like `HardSwish` with standard `ReLU`.\n",
    "*   **Why?** `HardSwish` is computationally expensive on simple MCUs and can cause issues with Int8 quantization simulation on PCs (XNNPACK bugs). Using the \"minimalistic\" version ensures better compatibility and efficiency for TinyML.\n",
    "\n",
    "**3. Simple CNN (Baseline)**\n",
    "*   **Overview:** A custom 3-layer Convolutional Neural Network.\n",
    "*   **Why included?** To establish a baseline. It proves that \"deep learning\" is actually necessary. A simple CNN often fails to capture the complex textures and shapes of \"people in the wild,\" leading to poor generalization.\n",
    "\n",
    "**Other Potential Candidates**\n",
    "*   **SqueezeNet:** An older architecture that pioneered small model sizes but relies on complex connections that can be harder to quantize effectively.\n",
    "*   **EfficientNet-Lite:** A powerful family of models, but even the smallest (Lite0) is often too heavy (computationally) for low-power MCUs like the ESP32 compared to MobileNetV2-0.35.\n",
    "*   **MCUNet:** A cutting-edge \"TinyNAS\" solution that designs the model *and* the inference engine together. While superior, it requires a complex specialized toolchain (TinyEngine) rather than standard TFLite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb5b9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "# --- Helper 1: Calculate MACs (Multiply-Accumulate Operations) ---\n",
    "def get_flops(model):\n",
    "    try:\n",
    "        # Convert Keras model to Frozen Graph for profiling\n",
    "        full_model = tf.function(lambda x: model(x))\n",
    "        concrete_func = full_model.get_concrete_function(\n",
    "            tf.TensorSpec([1, 96, 96, 3], model.inputs[0].dtype))\n",
    "        frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            tf.graph_util.import_graph_def(graph_def, name='')\n",
    "            run_meta = tf.compat.v1.RunMetadata()\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "            \n",
    "            # Suppress heavy output\n",
    "            flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "            return flops.total_float_ops\n",
    "    except Exception as e:\n",
    "        print(f\"  [Warning] Could not calculate FLOPs: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- Helper 2: Estimate Peak RAM (Arena Size) ---\n",
    "def estimate_peak_ram(model, element_size=4):\n",
    "    \"\"\"\n",
    "    Estimates Peak RAM based on the largest activation layer.\n",
    "    element_size: 4 for Float32, 1 for Int8.\n",
    "    \"\"\"\n",
    "    max_mem_bytes = 0\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        try:\n",
    "            # Calculate Input Size\n",
    "            input_shape = layer.input_shape\n",
    "            if isinstance(input_shape, list): \n",
    "                in_bytes = sum([np.prod(s[1:]) for s in input_shape if s is not None])\n",
    "            else:\n",
    "                in_bytes = np.prod(input_shape[1:])\n",
    "                \n",
    "            # Calculate Output Size\n",
    "            output_shape = layer.output_shape\n",
    "            if isinstance(output_shape, list):\n",
    "                out_bytes = sum([np.prod(s[1:]) for s in output_shape if s is not None])\n",
    "            else:\n",
    "                out_bytes = np.prod(output_shape[1:])\n",
    "            \n",
    "            # Total memory active during this layer's operation\n",
    "            current_layer_mem = (in_bytes + out_bytes) * element_size\n",
    "            \n",
    "            if current_layer_mem > max_mem_bytes:\n",
    "                max_mem_bytes = current_layer_mem\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return max_mem_bytes / 1024 # Convert to KB\n",
    "\n",
    "# 1. MobileNetV2 (Selected)\n",
    "def create_mobilenet_v2():\n",
    "    # MobileNetV2 expects inputs in [-1, 1]\n",
    "    # We add a Rescaling layer to map [0, 255] -> [-1, 1]\n",
    "    # This ensures our single preprocessing pipeline (outputting [0, 255]) works for all models.\n",
    "    base = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(96, 96, 3), \n",
    "        include_top=False, \n",
    "        alpha=0.35, \n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base.trainable = False # Freeze for transfer learning\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(96, 96, 3)),\n",
    "        tf.keras.layers.Rescaling(1./127.5, offset=-1),\n",
    "        base,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 2. MobileNetV3-Small (Comparison)\n",
    "def create_mobilenet_v3():\n",
    "    # MobileNetV3 expects inputs in [0, 255] (handled internally by its own Rescaling layer)\n",
    "    # We use minimalistic=True to replace HardSwish with ReLU.\n",
    "    # This improves compatibility with TFLite Int8 quantization on PC simulators (XNNPACK)\n",
    "    # and is often preferred for very low-power MCUs.\n",
    "    base = tf.keras.applications.MobileNetV3Small(\n",
    "        input_shape=(96, 96, 3), \n",
    "        include_top=False, \n",
    "        weights='imagenet',\n",
    "        minimalistic=True \n",
    "    )\n",
    "    base.trainable = False\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(96, 96, 3)),\n",
    "        # No Rescaling needed for V3 (it has it built-in)\n",
    "        base,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 3. Simple CNN (Baseline)\n",
    "def create_simple_cnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(96, 96, 3)),\n",
    "        tf.keras.layers.Rescaling(1./127.5, offset=-1), # Normalize to [-1, 1]\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "print(\"✅ Model definitions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f234ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary of models to train\n",
    "# We use the functions defined in the previous cell\n",
    "models_to_train = {\n",
    "    \"MobileNetV2\": create_mobilenet_v2(),\n",
    "    \"MobileNetV3\": create_mobilenet_v3(),\n",
    "    \"SimpleCNN\": create_simple_cnn()\n",
    "}\n",
    "\n",
    "# Compile all models\n",
    "for name, model in models_to_train.items():\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(f\"✅ {name} compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27625ffd",
   "metadata": {},
   "source": [
    "### 4.1 Training\n",
    "We train for 10 epochs with `EarlyStopping` and `ModelCheckpoint` to save the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d17524c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "histories = {}\n",
    "all_results = [] # Global list to store all metrics (Float & Int8)\n",
    "\n",
    "# Helper to evaluate Keras model (Float32)\n",
    "def evaluate_keras_model(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for img, label in dataset:\n",
    "        pred = model.predict(img, verbose=0)\n",
    "        y_pred.extend((pred > 0.5).astype(int).flatten())\n",
    "        y_true.extend(label.numpy().flatten())\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return f1\n",
    "\n",
    "print(f\"Starting comparison training for {len(models_to_train)} models...\")\n",
    "\n",
    "for name, model in models_to_train.items():\n",
    "    print(f\"\\n{'='*20} Training {name} {'='*20}\")\n",
    "    \n",
    "    # Callbacks (Unique filename for each model)\n",
    "    checkpoint_path = f'best_model_{name}.keras'\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_accuracy'),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # --- 1. Calculate Float32 Metrics ---\n",
    "    # Load best weights\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Size\n",
    "    model.save(f\"temp_{name}.keras\")\n",
    "    size_kb = os.path.getsize(f\"temp_{name}.keras\") / 1024\n",
    "    os.remove(f\"temp_{name}.keras\")\n",
    "    \n",
    "    # MACs\n",
    "    flops = get_flops(model)\n",
    "    macs_m = (flops / 2) / 1e6\n",
    "    \n",
    "    # RAM\n",
    "    ram_kb = estimate_peak_ram(model, element_size=4) # Float32 = 4 bytes\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = evaluate_keras_model(model, val_ds)\n",
    "    \n",
    "    # Store results\n",
    "    histories[name] = {\n",
    "        'history': history.history,\n",
    "        'duration': duration,\n",
    "        'best_val_acc': max(history.history['val_accuracy']),\n",
    "        'model_path': checkpoint_path\n",
    "    }\n",
    "    \n",
    "    all_results.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Float32',\n",
    "        'F1 Score': f1,\n",
    "        'Size (KB)': size_kb,\n",
    "        'MACs (M)': macs_m,\n",
    "        'Peak RAM (KB)': ram_kb\n",
    "    })\n",
    "    \n",
    "    print(f\"  -> Float32 Metrics: F1={f1:.2f}, Size={size_kb:.0f}KB, MACs={macs_m:.1f}M, RAM={ram_kb:.1f}KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da63010",
   "metadata": {},
   "source": [
    "### 4.2 Training Visualization\n",
    "We visualize the training history to diagnose model behavior:\n",
    "1.  **Accuracy/Loss Curves:** Check for overfitting (large gap between train/val) and convergence speed.\n",
    "2.  **Size vs. Accuracy Trade-off:** A scatter plot to visualize the \"cost\" (size) of \"performance\" (accuracy). Ideally, we want models in the top-left corner (High Accuracy, Small Size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6e1f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization of Comparison ---\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 1. Accuracy Plot (Learning Curves)\n",
    "plt.subplot(1, 3, 1)\n",
    "for name, data in histories.items():\n",
    "    plt.plot(data['history']['val_accuracy'], label=f\"{name} (Best: {data['best_val_acc']:.2%})\")\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Loss Plot\n",
    "plt.subplot(1, 3, 2)\n",
    "for name, data in histories.items():\n",
    "    plt.plot(data['history']['val_loss'], label=name)\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. Size vs Accuracy Scatter Plot (Trade-off)\n",
    "plt.subplot(1, 3, 3)\n",
    "names = list(histories.keys())\n",
    "accs = [histories[n]['best_val_acc'] * 100 for n in names] # %\n",
    "\n",
    "# FIX: Get size from all_results (which stores it in KB) and convert to MB\n",
    "sizes = []\n",
    "for n in names:\n",
    "    # Find the Float32 result for this model in all_results\n",
    "    res = next((r for r in all_results if r['Model'] == n and r['Type'] == 'Float32'), None)\n",
    "    if res:\n",
    "        sizes.append(res['Size (KB)'] / 1024) # KB -> MB\n",
    "    else:\n",
    "        sizes.append(0)\n",
    "\n",
    "# Scatter plot\n",
    "for i, name in enumerate(names):\n",
    "    plt.scatter(sizes[i], accs[i], s=150, label=name)\n",
    "    plt.text(sizes[i], accs[i] + 0.5, f\"{name}\", ha='center')\n",
    "\n",
    "plt.title('Trade-off: Model Size vs Accuracy')\n",
    "plt.xlabel('Model Size (MB) [Float32]')\n",
    "plt.ylabel('Best Val Accuracy (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# Add some padding to margins\n",
    "plt.margins(0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83ce12",
   "metadata": {},
   "source": [
    "## 5. Quantization & TFLite Conversion\n",
    "This is the most critical step for Edge AI. We convert the Float32 Keras model into an **Int8 Quantized TFLite model**.\n",
    "\n",
    "### Why Int8?\n",
    "1.  **Size:** Reduces model size by 4x (32-bit -> 8-bit).\n",
    "2.  **Speed:** MCUs can execute integer operations much faster than floating-point.\n",
    "3.  **Compatibility:** Many DSPs and NPUs only support integer math.\n",
    "\n",
    "### Representative Dataset\n",
    "To quantize accurately, the converter needs to observe a small set of real images to estimate the dynamic range (Min/Max) of activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24dff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative Dataset Generator (Shared for all models)\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in train_ds.unbatch().batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "quantized_models = {}\n",
    "\n",
    "print(\"Starting Quantization...\")\n",
    "for name, data in histories.items():\n",
    "    print(f\"Quantizing {name}...\")\n",
    "    \n",
    "    # Load the best float model saved during training\n",
    "    float_model = tf.keras.models.load_model(data['model_path'])\n",
    "    \n",
    "    # Converter Setup\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(float_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save\n",
    "    tflite_filename = f'vww_{name}_int8.tflite'\n",
    "    with open(tflite_filename, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    size_kb = len(tflite_model) / 1024\n",
    "    quantized_models[name] = {\n",
    "        'path': tflite_filename,\n",
    "        'size_kb': size_kb\n",
    "    }\n",
    "    print(f\"  -> Saved {tflite_filename} ({size_kb:.2f} KB)\")\n",
    "\n",
    "# --- Visual Comparison of Model Sizes ---\n",
    "names = list(quantized_models.keys())\n",
    "sizes = [quantized_models[n]['size_kb'] for n in names]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(names, sizes, color=['#2196F3', '#FF9800', '#4CAF50'])\n",
    "plt.title('Model Size Comparison (Int8 TFLite)')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f} KB',\n",
    "             ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa641be6",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Simulation\n",
    "We verify the quantized model using the `tf.lite.Interpreter`. \n",
    "\n",
    "**Why Simulation?**\n",
    "We cannot run the `.tflite` model directly in Keras. The Interpreter acts as a virtual MCU on our PC, allowing us to:\n",
    "1.  Check if **Int8 quantization** caused a significant drop in accuracy.\n",
    "2.  Verify the input/output tensor shapes match our expectations.\n",
    "3.  Estimate inference## 6. Evaluation & Simulation\n",
    "We verify the quantized model using the `tf.lite.Interpreter`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1034a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Clear previous Int8 results to avoid duplicates on re-run\n",
    "if 'all_results' in globals():\n",
    "    all_results = [r for r in all_results if r['Type'] != 'Int8']\n",
    "else:\n",
    "    all_results = []\n",
    "\n",
    "def evaluate_tflite(tflite_file, dataset):\n",
    "    try:\n",
    "        # Attempt to initialize the TFLite interpreter\n",
    "        interpreter = tf.lite.Interpreter(model_path=tflite_file)\n",
    "        interpreter.allocate_tensors()\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  ⚠️ Simulation Error for {tflite_file}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Get quantization parameters\n",
    "    input_scale, input_zero_point = input_details[0]['quantization']\n",
    "    output_scale, output_zero_point = output_details[0]['quantization']\n",
    "\n",
    "    # Ensure scalar\n",
    "    if isinstance(input_scale, (list, np.ndarray)): input_scale = float(np.mean(input_scale))\n",
    "    if isinstance(output_scale, (list, np.ndarray)): output_scale = float(np.mean(output_scale))\n",
    "    if isinstance(input_zero_point, (list, np.ndarray)): input_zero_point = int(np.mean(input_zero_point))\n",
    "    if isinstance(output_zero_point, (list, np.ndarray)): output_zero_point = int(np.mean(output_zero_point))\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    latencies = []\n",
    "    \n",
    "    print(f\"  Evaluating {tflite_file}...\")\n",
    "    \n",
    "    for images, labels in dataset.unbatch():\n",
    "        img_np = images.numpy()\n",
    "        label_np = labels.numpy()\n",
    "        \n",
    "        # Quantize Input\n",
    "        scale = input_scale if input_scale != 0 else 1.0\n",
    "        img_q = (img_np / scale) + input_zero_point\n",
    "        img_q = np.clip(img_q, -128, 127).astype(np.int8)\n",
    "        input_data = np.expand_dims(img_q, axis=0)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        \n",
    "        start = time.time()\n",
    "        interpreter.invoke()\n",
    "        end = time.time()\n",
    "        latencies.append((end - start) * 1000) # ms\n",
    "        \n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Dequantize Output\n",
    "        prob = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
    "        prob_val = float(prob.flatten()[0])\n",
    "        \n",
    "        y_pred.append(1 if prob_val > 0.5 else 0)\n",
    "        y_true.append(int(label_np) if np.ndim(label_np) == 0 else int(label_np.flatten()[0]))\n",
    "            \n",
    "    avg_latency = np.mean(latencies)\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, f1, avg_latency\n",
    "\n",
    "# --- Final Evaluation & Visualization ---\n",
    "print(f\"Evaluating Quantized Models...\")\n",
    "\n",
    "for name in histories.keys():\n",
    "    if name not in quantized_models:\n",
    "        continue\n",
    "        \n",
    "    tflite_path = quantized_models[name]['path']\n",
    "    acc, f1, lat = evaluate_tflite(tflite_path, val_ds)\n",
    "    \n",
    "    if acc is None: continue\n",
    "\n",
    "    # Get Float32 MACs (Architecture doesn't change)\n",
    "    float_entry = next((item for item in all_results if item['Model'] == name and item['Type'] == 'Float32'), None)\n",
    "    macs_m = float_entry['MACs (M)'] if float_entry else 0\n",
    "    \n",
    "    # Calculate Int8 Metrics\n",
    "    size_kb = os.path.getsize(tflite_path) / 1024\n",
    "    \n",
    "    # Re-create model to estimate RAM for Int8 (element_size=1)\n",
    "    if name == \"MobileNetV2\": model = create_mobilenet_v2()\n",
    "    elif name == \"MobileNetV3\": model = create_mobilenet_v3()\n",
    "    else: model = create_simple_cnn()\n",
    "    \n",
    "    ram_kb = estimate_peak_ram(model, element_size=1) # Int8 = 1 byte\n",
    "    \n",
    "    all_results.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Int8',\n",
    "        'F1 Score': f1,\n",
    "        'Size (KB)': size_kb,\n",
    "        'MACs (M)': macs_m,\n",
    "        'Peak RAM (KB)': ram_kb\n",
    "    })\n",
    "    \n",
    "    print(f\"  -> {name} (Int8): F1={f1:.2f}, Size={size_kb:.0f}KB, RAM={ram_kb:.1f}KB\")\n",
    "\n",
    "# --- 7. Comprehensive Summary ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   FINAL EVALUATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results = df_results[['Model', 'Type', 'F1 Score', 'Size (KB)', 'MACs (M)', 'Peak RAM (KB)']]\n",
    "\n",
    "# Display Table\n",
    "print(df_results.to_markdown(index=False, floatfmt=\".2f\"))\n",
    "\n",
    "# --- Visualization: Improved Comparison (Grouped Bar Charts) ---\n",
    "def plot_comparison_bars(df):\n",
    "    # Define metrics and their \"Better\" direction for titles\n",
    "    metrics_config = [\n",
    "        ('F1 Score', 'Higher is Better'),\n",
    "        ('Size (KB)', 'Lower is Better'),\n",
    "        ('MACs (M)', 'Lower is Better'),\n",
    "        ('Peak RAM (KB)', 'Lower is Better')\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    for i, (metric, direction) in enumerate(metrics_config):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        # Create bar plot using Seaborn\n",
    "        # hue='Type' separates Float32 vs Int8 side-by-side\n",
    "        sns.barplot(data=df, x='Model', y=metric, hue='Type', palette='viridis')\n",
    "        \n",
    "        plt.title(f\"{metric}\\n({direction})\", fontsize=12, fontweight='bold')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(title='Type')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        ax = plt.gca()\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.2f', padding=3, fontsize=9)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   VISUALIZATION: METRIC COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# We use the raw df_results, no normalization needed for bar charts\n",
    "plot_comparison_bars(df_results)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\" - F1 Score: Look for the HIGHEST bars. This measures detection quality.\")\n",
    "print(\" - Size/MACs/RAM: Look for the LOWEST bars. These measure efficiency.\")\n",
    "print(\" - Compare 'Float32' vs 'Int8' side-by-side to see the impact of quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ca0eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. Conclusion\n",
    "We successfully:\n",
    "1.  Acquired and cleaned the INRIA Person dataset.\n",
    "2.  Trained a MobileNetV2 model with **>90% accuracy**.\n",
    "3.  Quantized it to **Int8**, reducing size to **~600KB** with negligible accuracy loss.\n",
    "\n",
    "This model is now ready to be embedded into a smart camera system!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbcdcf",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Future Work (Phase 2)\n",
    "In this notebook (Phase 1), we successfully designed, trained, and quantized models, evaluating them using proxy metrics on a PC.\n",
    "\n",
    "**Phase 2 will focus on Hardware-in-the-Loop deployment:**\n",
    "1.  **Hardware Selection:** Procure ESP32-S3 or Arduino Nano 33 BLE Sense boards.\n",
    "2.  **On-Device Profiling:**\n",
    "    *   Measure **Real Latency** (using internal timers).\n",
    "    *   Measure **Power Consumption** (using current sensors).\n",
    "3.  **Optimization:**\n",
    "    *   Explore **CMSIS-NN** kernels for further acceleration on ARM Cortex-M chips.\n",
    "    *   Test **ESP-NN** for ESP32 optimization.\n",
    "4.  **Application Logic:** Implement the \"Wake Word\" logic (e.g., trigger an interrupt pin when a person is detected for >3 consecutive frames).\n",
    "\n",
    "\n",
    "#### Hardware Recommendations\n",
    "\n",
    "1.  **ESP32-CAM (Recommended):**\n",
    "    *   **Why:** Low cost, built-in camera, 4MB PSRAM handles the model easily.\n",
    "    *   **Deployment:** Use Arduino IDE with `ESP32` board support and `TFLite_Micro` library.\n",
    "2.  **Arduino Nano 33 BLE Sense:**\n",
    "    *   **Why:** Standard TinyML education board, rich sensors.\n",
    "    *   **Note:** Requires external camera. RAM (256KB) is tight for this kind of models; might need further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ad269",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* https://arxiv.org/html/2506.18927v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
